{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 经纬度及API调用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 经度(longitude):东西方向为经，范围是[-180,180]\n",
    "* 纬度(latitude)：南北方向为纬，范围是[-90,90]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 经纬度编码格式转换&求距离"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[坐标系参考](https://www.jianshu.com/p/01f1bc49ba97)\n",
    "\n",
    "\n",
    "* **WGS－84原始坐标系**:一般用国际GPS纪录仪记录下来的经纬度，通过GPS定位拿到的原始经纬度，Google和高德地图定位的的经纬度（国外）都是基于WGS－84坐标系的；\n",
    "    \n",
    "    但是在国内是不允许直接用WGS84坐标系标注的，必须经过加密后才能使用；\n",
    "\n",
    "\n",
    "* **GCJ－02坐标系**:又名“火星坐标系”，是我国国测局独创的坐标体系，由**WGS－84**加密而成。在国内必须使用GCJ－02坐标系或者使用GCJ－02加密后再进行加密成 **bd-09** 的坐标系，如百度坐标系。\n",
    "\n",
    "    高德和Google在国内都是使用GCJ－02坐标系，可以说GCJ－02是国内最广泛使用的坐标系；\n",
    "\n",
    "\n",
    "* **百度坐标系(bd-09)**:百度坐标系是在GCJ－02坐标系的基础上再次加密偏移后形成的坐标系，只适用于百度地图。\n",
    "\n",
    "    目前百度API提供了从其它坐标系转换为百度坐标系的API，但却没有从百度坐标系转为其他坐标系的API。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面是坐标系转换代码，包含：\n",
    "\n",
    "|原坐标系X|转换函数|转换后的坐标系Y|Y类坐标系|\n",
    "| :--    |:--    |:--         |:--      |\n",
    "|WGS-84  |wgs_gcj_encrypt|GCJ-02|高德 和 Google 坐标系|\n",
    "|GCJ-02  |gcj_bd_encrypt|bd-09|百度坐标系|\n",
    "|WGS-84  |wgs_baidu_encrypt|bd-09|百度坐标系|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T03:28:59.298879Z",
     "start_time": "2019-05-14T03:28:59.262877Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# 我们拿到的TBOX的经纬度有一点点偏差，大概十几二十米的样子。\n",
    "# 用在一般的分析上不影响，但如果画到地图上，会出现像车子在黄浦江里的情景\n",
    "# 调用wgs_baidu_encrypt方法，传入一个latitude,longitude，以list返回纠偏后的lat和lon。\n",
    "\n",
    "import math\n",
    "x_pi = math.pi * 3000.0 / 180.0\n",
    "AA = 6378245.0\n",
    "EE = 0.00669342162296594323\n",
    "r = 6371\n",
    "# ------------- basic functions -------------------\n",
    "\n",
    "\n",
    "def distance_of_two_points(point_1, point_2):  \n",
    "    '''\n",
    "    计算两个经纬度点之间的距离\n",
    "    type(point) = list\n",
    "    point = [lat, lon]\n",
    "\n",
    "    '''\n",
    "    lat1, lon1, lat2, lon2 = map(math.radians, point_1+point_2)\n",
    "\n",
    "    diff_lon = lon2 - lon1\n",
    "    diff_lat = lat2 - lat1\n",
    "    a = math.sin(diff_lat / 2) ** 2 + math.cos(lat1) * math.cos(lat2) * math.sin(diff_lon / 2) ** 2\n",
    "    c = 2 * math.asin(math.sqrt(a))\n",
    "    return c * r * 1000\n",
    "\n",
    "'''\n",
    "#  * 经纬度纠偏\n",
    "#  * wgs_lat 纬度\n",
    "#  * wgs_lon 经度\n",
    "#  * GCJ-02转换BD-09\n",
    "#  * 高德和Google地图经纬度转百度地图经纬度\n",
    "'''\n",
    "\n",
    "\n",
    "def wgs_baidu_encrypt(wgs_lat, wgs_lon):\n",
    "    tmp_lat_lon = wgs_gcj_encrypts(wgLat=wgs_lat, wgLon=wgs_lon)\n",
    "    return gcj_bd_encrypt(tmp_lat_lon[0], tmp_lat_lon[1])\n",
    "\n",
    "        \n",
    "'''\n",
    "#  * gg_lat 纬度\n",
    "#  * gg_lon 经度\n",
    "#  * GCJ-02转换BD-09\n",
    "#  * 高德和Google地图经纬度转百度地图经纬度\n",
    "'''\n",
    "\n",
    "\n",
    "def gcj_bd_encrypt(gg_lat, gg_lon):\n",
    "    x = gg_lon\n",
    "    y = gg_lat\n",
    "    z = math.sqrt(x * x + y * y) + 0.00002 * math.sin(y * x_pi)\n",
    "    theta = math.atan2(y, x) + 0.000003 * math.cos(x * x_pi)\n",
    "    bd_lon = z * math.cos(theta) + 0.0065\n",
    "    bd_lat = z * math.sin(theta) + 0.006\n",
    "    return [round(bd_lat, 4), round(bd_lon, 4)]\n",
    "\n",
    "\n",
    "'''\n",
    "#  * wgLat 纬度\n",
    "#  * wgLon 经度\n",
    "#  * BD-09转换GCJ-02\n",
    "#  * 百度转google\n",
    "'''\n",
    "def bd_google_encrypt(bd_lat, bd_lon):\n",
    "    x = bd_lon - 0.0065\n",
    "    y = bd_lat - 0.006;  \n",
    "    z = math.sqrt(x * x + y * y) - 0.00002 * math.sin(y * x_pi)  \n",
    "    theta = math.atan2(y, x) - 0.000003 * math.cos(x * x_pi) \n",
    "    gg_lon = z * math.cos(theta)  \n",
    "    gg_lat = z * math.sin(theta) \n",
    "    return gg_lat, gg_lon\n",
    "\n",
    "\n",
    "'''\n",
    "#  * wgLat 纬度\n",
    "#  * wgLon 经度\n",
    "#  * WGS-84 到 GCJ-02 的转换（即 GPS 加偏）\n",
    "'''\n",
    "def wgs_gcj_encrypts(wgLat, wgLon):\n",
    "    if outOfChina(wgLat, wgLon):\n",
    "        return wgLat, wgLon\n",
    "        \n",
    "    dLat = transformLat(wgLon - 105.0, wgLat - 35.0)\n",
    "    dLon = transformLon(wgLon - 105.0, wgLat - 35.0)\n",
    "    radLat = wgLat / 180.0 * math.pi\n",
    "    magic = math.sin(radLat)\n",
    "    magic = 1 - EE * magic * magic\n",
    "    sqrtMagic = math.sqrt(magic)\n",
    "    dLat = (dLat * 180.0) / ((AA * (1 - EE)) / (magic * sqrtMagic) * math.pi)\n",
    "    dLon = (dLon * 180.0) / (AA / sqrtMagic * math.cos(radLat) * math.pi)\n",
    "    lat = wgLat + dLat\n",
    "    lon = wgLon + dLon\n",
    "    return lat, lon\n",
    "\n",
    "\n",
    "\n",
    "def outOfChina(lat, lon):  \n",
    "    if lon < 72.004 or lon > 137.8347:\n",
    "        return True\n",
    "    if lat < 0.8293 or lat > 55.8271:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def transformLat(x, y):\n",
    "    ret = -100.0 + 2.0 * x + 3.0 * y + 0.2 * y * y + 0.1 * x * y + 0.2 * math.sqrt(abs(x))\n",
    "    ret = ret + (20.0 * math.sin(6.0 * x * math.pi) + 20.0 * math.sin(2.0 * x * math.pi)) * 2.0 / 3.0\n",
    "    ret = ret + (20.0 * math.sin(y * math.pi) + 40.0 * math.sin(y / 3.0 * math.pi)) * 2.0 / 3.0\n",
    "    ret = ret + (160.0 * math.sin(y / 12.0 * math.pi) + 320 * math.sin(y * math.pi / 30.0)) * 2.0/3.0\n",
    "    return ret\n",
    "\n",
    "\n",
    "def transformLon(x, y):\n",
    "    ret = 300.0 + x + 2.0 * y + 0.1 * x * x + 0.1 * x * y + 0.1 * math.sqrt(abs(x))\n",
    "    ret = ret + (20.0 * math.sin(6.0 * x * math.pi) + 20.0 * math.sin(2.0 * x * math.pi)) * 2.0 / 3.0\n",
    "    ret = ret + (20.0 * math.sin(x * math.pi) + 40.0 * math.sin(x / 3.0 * math.pi)) * 2.0 / 3.0\n",
    "    ret = ret + (150.0 * math.sin(x / 12.0 * math.pi) + 300.0 * math.sin(x / 30.0 * math.pi)) * 2.0/3.0\n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T03:28:59.923915Z",
     "start_time": "2019-05-14T03:28:59.905914Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[121.3317, 31.0628]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wgs_baidu_encrypt(wgs_lat=121.325621, wgs_lon=31.056428)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T03:42:42.721007Z",
     "start_time": "2019-05-14T03:42:42.714006Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5248.999011991885"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance_of_two_points(point_1=[121.3256,31.0564], point_2=[121.2814, 31.0245])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 区域划分网格"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T07:30:42.862000Z",
     "start_time": "2018-12-05T07:30:41.722000Z"
    },
    "code_folding": [
     15,
     29,
     55,
     77,
     97,
     117,
     157
    ],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu May 16 2018\n",
    "\n",
    "@author: bofeng chen\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import random\n",
    "error_sign = '-1'\n",
    "SH_latitude_range = (30.65, 31.89)\n",
    "SH_longitude_range = (120.85, 122.04)\n",
    "\n",
    "\n",
    "def area_split_num(left_point, right_point, diameter):\n",
    "    \"\"\"\n",
    "    输入区域的lat/lon的范围和半径，输出网格数量\n",
    "    比例尺度 0.01 == 1000miles\n",
    "    :param left_point: lat/lon边的左端点\n",
    "    :param right_point: lat/lon边的右端点\n",
    "    :param diameter: int, 一个网格的半径\n",
    "    :return: 网格的lat/lon边的分割网格数量\n",
    "    \"\"\"\n",
    "    # 取area的长边、宽边的各自两个点\n",
    "    lat_lon_len = diameter/float(1000)*0.01\n",
    "    return int((right_point-left_point)/lat_lon_len)\n",
    "\n",
    "\n",
    "def area_basic_para(latitude_tuple, longitude_tuple, diameter):\n",
    "    \"\"\"\n",
    "    输入area范围和网格直径，输出网格基本信息\n",
    "    :param latitude_tuple: tuple, area的纬度范围\n",
    "    :param longitude_tuple: tuple, area的经度范围\n",
    "    :param diameter: int, 直径\n",
    "    :return: 区域lat/lon的基本参数，包括area的范围，对应的网格数量，网格单位长度；编码位数\n",
    "    \"\"\"\n",
    "    # 网格的四个角\n",
    "    area_left = longitude_tuple[0]\n",
    "    area_right = longitude_tuple[1]\n",
    "    area_down = latitude_tuple[0]\n",
    "    area_up = latitude_tuple[1]\n",
    "\n",
    "    split_num_list = map(lambda x: area_split_num(left_point=x[0], right_point=x[1], diameter=diameter),\n",
    "                         [[area_left, area_right], [area_down, area_up]])\n",
    "\n",
    "    encode_len = max([len(str(i)) for i in split_num_list])\n",
    "\n",
    "    unit_len_list = [(longitude_tuple[1] - longitude_tuple[0])/split_num_list[0], (latitude_tuple[1] - latitude_tuple[0])/split_num_list[1]]\n",
    "    longitude_dict = {'range': longitude_tuple, 'split_num': split_num_list[0], 'unit_len': unit_len_list[0]}\n",
    "    latitude_dict = {'range': latitude_tuple, 'split_num': split_num_list[1], 'unit_len': unit_len_list[1]}\n",
    "\n",
    "    return longitude_dict, latitude_dict, encode_len\n",
    "\n",
    "\n",
    "def create_a_grid_table(latitude_dict, longitude_dict, encode_len):\n",
    "    \"\"\"\n",
    "    输入区域基本参数，输出区域的网格编码表\n",
    "    :param latitude_dict: dict, 区域单边的基本参数\n",
    "    :param longitude_dict: dict, 区域单边的基本参数\n",
    "    :param encode_len: 编码长度\n",
    "    :return: DataFrame, 网格编码表\n",
    "    \"\"\"\n",
    "    lat_num = latitude_dict['split_num']\n",
    "    lon_num = longitude_dict['split_num']\n",
    "\n",
    "    grid_info_list = []\n",
    "    for lon_i in xrange(lon_num):\n",
    "        lon_center = longitude_dict['range'][0] + lon_i*longitude_dict['unit_len'] + longitude_dict['unit_len']/2\n",
    "        for lat_j in xrange(lat_num):\n",
    "            encode_id = ''.join(map(lambda x: x.zfill(encode_len), [str(lon_i), str(lat_j)]))\n",
    "            lat_center = latitude_dict['range'][0] + lat_j * longitude_dict['unit_len'] + latitude_dict['unit_len'] / 2\n",
    "            grid_info_list.append([encode_id, str(lon_i), str(lat_j), lon_center, lat_center])\n",
    "    grid_info_pd = pd.DataFrame(grid_info_list, columns=['grid_id', 'lon_cooridate', 'lat_cooridate', 'lon_center', 'lat_center'])\n",
    "    return grid_info_pd\n",
    "\n",
    "\n",
    "def area_side_encode_to_coordinate(side_para_dict, side_tude):\n",
    "    \"\"\"\n",
    "    给出相应lat/lon对应的网格坐标\n",
    "    :param side_para_dict: dict, 区域单边的基本参数\n",
    "    :param side_tude: float, latitude or longitude\n",
    "    :return: side_tude对应的坐标\n",
    "    \"\"\"\n",
    "\n",
    "    side_range = side_para_dict['range']\n",
    "    unit_len = side_para_dict['unit_len']\n",
    "    if side_range[0] <= side_tude <= side_range[1]:\n",
    "        # 进行编码\n",
    "        side_coordinate = int((side_tude - side_range[0])/unit_len)\n",
    "        if side_coordinate == side_para_dict['split_num']:\n",
    "            side_coordinate -= 1\n",
    "        return str(side_coordinate)\n",
    "    else:\n",
    "        return error_sign\n",
    "\n",
    "\n",
    "def area_encode_to_id(latitude_dict, longitude_dict, encode_len, latitude, longitude):\n",
    "    \"\"\"\n",
    "    输入区域的基本参数和一对经纬度，输出经纬度对应的网格id\n",
    "    :param latitude_dict: dict, 区域单边的基本参数\n",
    "    :param longitude_dict: dict, 区域单边的基本参数\n",
    "    :param encode_len: 编码长度\n",
    "    :param latitude: 纬度\n",
    "    :param longitude: 经度\n",
    "    :return: 经纬度对应的网格id\n",
    "    \"\"\"\n",
    "    lat_cooridate = area_side_encode_to_coordinate(side_para_dict=latitude_dict, side_tude=latitude)\n",
    "    lon_cooridate = area_side_encode_to_coordinate(side_para_dict=longitude_dict, side_tude=longitude)\n",
    "\n",
    "    if lat_cooridate != error_sign and lon_cooridate != error_sign:\n",
    "        encode_id = ''.join(map(lambda x: x.zfill(encode_len), [lon_cooridate, lat_cooridate]))\n",
    "    else:\n",
    "        encode_id = error_sign\n",
    "    return encode_id\n",
    "\n",
    "\n",
    "def random_choose_grid(grid_news_path, need_points_num, percent_list, random_times):\n",
    "    \"\"\"\n",
    "    输入候选所需网格数量、网格数据在不同分位数下的分布，输出是比赛所需的网格id及相关基础信息\n",
    "    :param grid_news_path: str，根据500米/1000米为半径的网格编码方式得到的不同分位数的网格以及9~22点之间相应周平均车流量数据csv路径\n",
    "    :param need_points_num: int,比赛一共需要的网格数量\n",
    "    :param percent_list: list, 各个分位数所需网格数量的百分比\n",
    "    :param random_times: int,循环次数\n",
    "    :return: DataFrame，选取到的网格id信息以及对应的中心点经纬度以及其他字段数据\n",
    "    \"\"\"\n",
    "\n",
    "    grid_news = pd.read_csv(grid_news_path, dtype={'grid_id': str})\n",
    "    need_news = grid_news[['percent_level', 'grid_id']].drop_duplicates()\n",
    "\n",
    "    sum_0 = 0\n",
    "    id_list = 0\n",
    "    for x in xrange(random_times):\n",
    "        print sum_0\n",
    "        grid_list = []\n",
    "        for label, label_df in need_news.groupby('percent_level'):\n",
    "            num = int(percent_list[int(label)]*need_points_num)\n",
    "            label_id_list = list(label_df['grid_id'])\n",
    "            random.shuffle(label_id_list)\n",
    "            grid_list.extend(label_id_list[0:num])\n",
    "\n",
    "        combin_list = list(itertools.combinations(grid_list, 2))\n",
    "\n",
    "        def distiance_two_int_str(str_tuple):\n",
    "            int_list = map(lambda x: [int(x[0:3]), int(x[3:])], str_tuple)\n",
    "            return abs(int_list[0][0] - int_list[1][0]) + abs(int_list[0][1] - int_list[1][1])\n",
    "\n",
    "        sum_1 = sum(map(lambda x: distiance_two_int_str(x), combin_list))\n",
    "        if sum_1 > sum_0:\n",
    "            sum_0 = sum_1\n",
    "            id_list = grid_list\n",
    "\n",
    "    choose_grid_df = grid_news[grid_news.grid_id.isin(id_list)]\n",
    "    df = choose_grid_df[['grid_id', 'lat_center', 'lon_center']].drop_duplicates()\n",
    "    return choose_grid_df, df\n",
    "\n",
    "\n",
    "def game_50_grid_basic_information(id_info_df, grid_type):\n",
    "    \"\"\"\n",
    "    输入是网格id和中心经纬度，输出是网格encode后的id以及网格的边界信息\n",
    "    :param id_info_df: DataFrame，字段包含网格id以及对应的中心点经纬度\n",
    "    :param grid_type: int, 可选500 或 1000\n",
    "    :return: DataFrame, 网格id和对应网格的lat上下边界、lon左右边界以及id相应的encode后的id\n",
    "    \"\"\"\n",
    "    assert (grid_type == 500 or grid_type == 1000)\n",
    "    diameter = grid_type/100000.0\n",
    "    id_info_df['down_lat'] = id_info_df[['lat_center']].apply(lambda x: x-diameter/2)\n",
    "    id_info_df['up_lat'] = id_info_df[['lat_center']].apply(lambda x: x + diameter / 2)\n",
    "    id_info_df['left_lon'] = id_info_df[['lon_center']].apply(lambda x: x - diameter / 2)\n",
    "    id_info_df['right_lon'] = id_info_df[['lon_center']].apply(lambda x: x + diameter / 2)\n",
    "    id_info_df['encode_grid_id'] = range(id_info_df.shape[0])\n",
    "    return id_info_df\n",
    "\n",
    "# 要写入sql脚本的经纬度归属网格的计算公式\n",
    "# 当半径定位500米时，\n",
    "# 网格id的计算公式是：\n",
    "# lon_cooridate = int((longitude - 120.85)/0.005)\n",
    "# lat_cooridate = int((latitude - 30.65)/0.005)\n",
    "# 对应的网格中心计算公式：\n",
    "# center_longtitude = 120.85 + lon_coordinate * 0.005 +0.005 / 2\n",
    "# center_latitude = 30.65 + lat_coordinate * 0.005 + 0.005 / 2\n",
    "#\n",
    "# 当半径定位1000米时，\n",
    "# 网格id的计算公式是：\n",
    "# lon_cooridate = int((longitude - 120.85)/0.01)\n",
    "# lat_cooridate = int((latitude - 30.65)/0.01)\n",
    "# 对应的网格中心计算公式：\n",
    "# center_longtitude = 120.85 + lon_coordinate * 0.01 +0.01 / 2\n",
    "# center_latitude = 30.65 + lat_coordinate * 0.01 + 0.01 / 2\n",
    "\n",
    "\n",
    "# 本地测试\n",
    "# if __name__ == \"__main__\":\n",
    " \n",
    "#     # 测试网格编码表生成和经纬度所属网格id\n",
    "#     longitude_dict, latitude_dict, encode_len = area_basic_para(latitude_tuple=SH_latitude_range, longitude_tuple=SH_longitude_range, diameter=1000)\n",
    "#     grid_info_pd = create_a_grid_table(latitude_dict=latitude_dict, longitude_dict=longitude_dict, encode_len=encode_len)\n",
    "#     # encode_id = area_encode_to_id(latitude_dict=latitude_dict, longitude_dict=longitude_dict, encode_len=encode_len, latitude=31.89, longitude=121.85)\n",
    "\n",
    "#     # # 根据候选网格选取比赛所需网格\n",
    "#     # choose_grid_df, df = random_choose_grid(\n",
    "#     #     grid_news_path='input/hour_percent_grid_data_500miles_1000vins_e550_20170102_20170108.csv',\n",
    "#     #     need_points_num=50, percent_list=[0.2, 0.2, 0.3, 0.3], random_times=100000)\n",
    "#     # choose_grid_df.to_csv('output/50_grid_heat.csv')\n",
    "#     game_50_grid_basic_information(id_info_df=grid_info_pd, grid_type=500).to_csv('output/encode_50_grid_basic_info.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 求站点之间距离和时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T07:14:28.064897Z",
     "start_time": "2018-12-05T07:14:27.761754Z"
    }
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'XA_Location.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-5133dc0ee968>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdistance_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mlocation_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_csv_file_for_order_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'XA_Location.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0mdistance_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_distance_and_time_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-5133dc0ee968>\u001b[0m in \u001b[0;36mread_csv_file_for_order_location\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#读取XA站点和其经纬度\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_csv_file_for_order_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0minput_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mlocation_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'XA_Location.csv'"
     ]
    }
   ],
   "source": [
    "import urllib2\n",
    "import csv\n",
    "import json\n",
    "\n",
    "\n",
    "#读取XA站点和其经纬度\n",
    "def read_csv_file_for_order_location(path):\n",
    "    input_file = csv.reader(file(path))\n",
    "    location_dict = dict()\n",
    "    for i in input_file:\n",
    "        location_dict[i[0]] = [float(i[1]), float(i[2])]\n",
    "    return location_dict\n",
    "\n",
    "\n",
    "#输入：站点的经纬度\n",
    "#输出：站点之间的驾车模式的路线距离和时间\n",
    "#主要通过调用Route Matrix API v2.0来得到输出\n",
    "def get_distance_and_time_matrix(location_dict):\n",
    "    distance_data = []\n",
    "\n",
    "    for ID_1 in location_dict.keys():\n",
    "        for ID_2 in location_dict.keys():\n",
    "            if ID_2 != ID_1:\n",
    "                Ori_Des = (location_dict[ID_1][0], location_dict[ID_1][1], location_dict[ID_2][0], location_dict[ID_2][1],\n",
    "                           location_dict[ID_1][0], location_dict[ID_1][1], location_dict[ID_2][0], location_dict[ID_2][1])\n",
    "                http = 'http://api.map.baidu.com/routematrix/v2/driving?output=json&origins=%2.6f,%3.6f|%2.6f,%3.6f&destinations=%2.6f,%3.6f|%2.6f,%3.6f&ak=Mv58QwKzsPcV43H9sMtUDjIe0D84FwDz' % Ori_Des\n",
    "                request = urllib2.Request(http)\n",
    "                f = urllib2.urlopen(request)\n",
    "                data = f.read()\n",
    "                js = json.loads(data)\n",
    "                distance_time = ['TRUE',\n",
    "                                 float(js[u'result'][1][u'distance'][u'value'])/1000,\n",
    "                                 float(js[u'result'][1][u'duration'][u'value'])/3600,\n",
    "                                 'TRUE',\n",
    "                                 float(js[u'result'][2][u'distance'][u'value'])/1000,\n",
    "                                 float(js[u'result'][2][u'duration'][u'value'])/3600]\n",
    "                ID = [ID_1, ID_2]\n",
    "                ID.extend(distance_time)\n",
    "                distance_data.append(ID)\n",
    "\n",
    "    csv_file = open('distance.csv', 'wb')\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerows(distance_data)\n",
    "    csv_file.close()\n",
    "    return distance_data\n",
    "\n",
    "location_dict = read_csv_file_for_order_location('XA_Location.csv')\n",
    "distance_data = get_distance_and_time_matrix(location_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [逆地理编码](http://lbsyun.baidu.com/index.php?title=webapi/guide/webservice-geocoding-abroad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**逆地理编码服务**：用户可通过该功能，将位置坐标解析成对应的行政区划数据以及周边高权重地标地点分布情况，整体描述坐标所在的位置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T11:52:31.504000Z",
     "start_time": "2018-12-05T11:52:30.490000Z"
    }
   },
   "outputs": [],
   "source": [
    "# coding:utf-8\n",
    "import urllib2\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "def desc_getpois(api_input_dict, add_columns_list):\n",
    "    \n",
    "    if 'extensions_poi' in api_input_dict and api_input_dict['extensions_poi'] is 'null':\n",
    "        assert api_input_dict['pois'] == 0\n",
    "    \n",
    "    lat = api_input_dict['location']['latitude']\n",
    "    lon = api_input_dict['location']['longitude']\n",
    "    param_str_list = []\n",
    "    for key, values in api_input_dict.items():\n",
    "        if key is not 'location':\n",
    "            param_str_list.append(key + '=' + str(values))\n",
    "        else:\n",
    "            param_str_list.append(key + '=' + str(lat) + ',' + str(lon))\n",
    "     \n",
    "    param_str = '&'.join(param_str_list)\n",
    "\n",
    "    desc_geocoder_api = 'http://api.map.baidu.com/geocoder/v2/?{0}'.format(param_str)\n",
    "    info_dict = json.loads(urllib2.urlopen(desc_geocoder_api, timeout=3).read())\n",
    "    if info_dict['status'] == 0:\n",
    "        return {key: info_dict['result']['addressComponent'][key] for key in ['province', 'city', 'district']}\n",
    "    else:\n",
    "        return {key: 'call_api_failed' for key in ['province', 'city', 'district']}\n",
    "\n",
    "    \n",
    "   \n",
    "def df_call_baidu_geocoder_api(ak_df, lat_lon_df, latlon_columns_name_list, add_columns_list):\n",
    "    \n",
    "    lat_lon_df['province'] = ''\n",
    "    lat_lon_df['city'] = ''\n",
    "    lat_lon_df['district'] = ''\n",
    "    \n",
    "    lat_lon_df.reset_index(drop=True, inplace=True)\n",
    "    for index in tqdm(lat_lon_df.index):\n",
    "        ak_index = index% ak_df.shape[0]\n",
    "        ak = ak_df.iloc[ak_index, 1]\n",
    "        lat = lat_lon_df.loc[index, latlon_columns_name_list[0]]\n",
    "        lon = lat_lon_df.loc[index, latlon_columns_name_list[1]]\n",
    "        api_input_dict = {'location': {'latitude': lat, 'longitude': lon}, \n",
    "                    'pois':0,'ak':ak, 'output': 'json', 'extensions_poi': 'null', 'language': 'en'}\n",
    "        info_dict = desc_getpois(api_input_dict=api_input_dict, add_columns_list=add_columns_list)\n",
    "        lat_lon_df.loc[index, add_columns_list] = [info_dict[col] for col in add_columns_list]\n",
    "    return lat_lon_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_input_dict = {'location': {'latitude': lat, 'longitude': lon}, \n",
    "                    'pois':0,'ak':ak, 'output': 'json', 'extensions_poi': 'null', 'language': 'en'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [地理编码](http://lbsyun.baidu.com/index.php?title=webapi/guide/webservice-geocoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**地理编码服务**:用户可通过该功能，将结构化地址（省/市/区/街道/门牌号）解析为对应的位置坐标。地址结构越完整，地址内容越准确，解析的坐标精度越高。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`http://api.map.baidu.com/geocoder/v2/?address=北京市海淀区上地十街10号&output=json&ak=您的ak&callback=showLocation //GET请求`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T03:21:13.892656Z",
     "start_time": "2019-06-05T03:21:13.872656Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# coding:utf-8\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "output_type = 'json'\n",
    "ak = 'joEZrGxLI7hH0Bggf7rVBUPwB9hgooyx'\n",
    "city = '上海市'\n",
    "def getpois(address):\n",
    "    \n",
    "    getpois_api = 'http://api.map.baidu.com/geocoder/v2/?address={0}&output={1}&ak={2}&city={3}'.format(address,output_type,ak,city)\n",
    "    info_dict = requests.get(getpois_api).json()\n",
    "    \n",
    "    precise = info_dict['result']['precise']\n",
    "    confidence = info_dict['result']['confidence']\n",
    "    comprehension = info_dict['result']['comprehension']\n",
    "    level = info_dict['result']['level']\n",
    "    lng = info_dict['result']['location']['lng']\n",
    "    lat = info_dict['result']['location']['lat']\n",
    "    return [address , lat, lng, precise, confidence, comprehension]\n",
    "\n",
    "\n",
    "def df_add_poi(in_path,out_path):\n",
    "    info_list = []\n",
    "    old_df = pd.read_csv(path)\n",
    "    for i in old_df['store_address']:\n",
    "        info_list.append(getpois(address=i))\n",
    "        info_df = pd.DataFrame(info_list,columns=['store_address', 'baidu_lat', 'baidu_lng','precise', 'confidence','comprehension'])\n",
    "        new_df = old_df.merge(info_df,on='store_address',how='inner')\n",
    "        new_df.to_csv(out_path,index=False,encoding='utf-8')\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T03:21:14.743696Z",
     "start_time": "2019-06-05T03:21:14.624690Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['上海市杨浦区吉浦路3号', 31.30492357706552, 121.49789529042646, 1, 80, 100]\n"
     ]
    }
   ],
   "source": [
    "info_list = getpois(address='上海市杨浦区吉浦路3号')\n",
    "print(info_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 返回指定线路交通态势"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T07:02:02.939000Z",
     "start_time": "2019-03-26T07:02:02.901000Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_load_info_rectangle(ak, left_lon, left_lat, right_lon, right_lat):\n",
    "    '''\n",
    "    返回矩形区域交通态势\n",
    "    ak:密钥\n",
    "    left_lon:矩形左下角的longitude\n",
    "    left_lat: 矩形左下角的latitude\n",
    "    right_lon:矩形右上角的longitude\n",
    "    right_lat:矩阵右上角的latitude\n",
    "    Note:矩阵对角线要小于10km\n",
    "    '''\n",
    "    if distance_of_two_points([left_lat, left_lon], [right_lat, right_lon]) > 10000:\n",
    "        print \"The diagonal of the rectangle should be less than  10km\"\n",
    "        return None\n",
    "    url = \"https://restapi.amap.com/v3/traffic/status/rectangle?rectangle=\" + \\\n",
    "    \"\".join([str(left_lon), \",\", str(left_lat), \";\", \\\n",
    "             str(right_lon), \",\", str(right_lat), \"&key=\", str(ak)])\n",
    "    try:\n",
    "        info = urllib.urlopen(url).read()\n",
    "        return info\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_load_info_circle(ak, center_lon, center_lat, radius):\n",
    "    '''\n",
    "    返回圆形区域交通态势\n",
    "    ak:密钥\n",
    "    center_lon:该圆形区域中心点longitude\n",
    "    center_lat:该圆形区域中心的latitude\n",
    "    radius:该圆形区域对应的半径\n",
    "    Note:radius要小于5000米\n",
    "    '''\n",
    "    if radius > 5000:\n",
    "        print \"radius should be less than 5000\"\n",
    "        return None\n",
    "    url = \"https://restapi.amap.com/v3/traffic/status/circle?location=\" + \\\n",
    "    \"\".join([str(center_lon), \",\", str(center_lat), \"&radius=\", str(radius), \"&key=\", str(ak)])\n",
    "    try:\n",
    "        info = urllib.urlopen(url).read()\n",
    "        return info\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_load_info_road(ak, name, city=None, adcode=None):\n",
    "    '''\n",
    "    返回指定线路交通态势\n",
    "    ak:密钥\n",
    "    name:道路名\n",
    "    city:城市名\n",
    "    adcode:城市编码\n",
    "    Note:city和adcode要填其中一个即可\n",
    "    '''\n",
    "    if city is None and adcode is None:\n",
    "        print \"please specify city or adcode\"\n",
    "        return None\n",
    "    else:            \n",
    "        if city is not None:\n",
    "            url = \"https://restapi.amap.com/v3/traffic/status/road?name=\"+\\\n",
    "            \"\".join([name,\"&city=\",city,\"&key=\",str(ak)])\n",
    "        else:\n",
    "            url = \"https://restapi.amap.com/v3/traffic/status/road?name=\"+\\\n",
    "            \"\".join([name,\"&adcode=\",str(adcode),\"&key=\",str(ak)])  \n",
    "        try:\n",
    "            info = urllib.urlopen(url).read()\n",
    "            return info    \n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 爬虫类代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BeautifulSoup4.2.0文档练习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T06:38:55.869000Z",
     "start_time": "2019-03-26T06:38:55.847000Z"
    }
   },
   "outputs": [],
   "source": [
    "html_doc = \"\"\"\n",
    "<html><head><title>The Dormouse's story</title></head>\n",
    "<body>\n",
    "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
    "\n",
    "<p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
    "<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>,\n",
    "<a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\n",
    "<a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\n",
    "and they lived at the bottom of a well.</p>\n",
    "\n",
    "<p class=\"story\">...</p>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T06:38:56.703000Z",
     "start_time": "2019-03-26T06:38:56.663000Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      " <head>\n",
      "  <title>\n",
      "   The Dormouse's story\n",
      "  </title>\n",
      " </head>\n",
      " <body>\n",
      "  <p class=\"title\">\n",
      "   <b>\n",
      "    The Dormouse's story\n",
      "   </b>\n",
      "  </p>\n",
      "  <p class=\"story\">\n",
      "   Once upon a time there were three little sisters; and their names were\n",
      "   <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">\n",
      "    Elsie\n",
      "   </a>\n",
      "   ,\n",
      "   <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">\n",
      "    Lacie\n",
      "   </a>\n",
      "   and\n",
      "   <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">\n",
      "    Tillie\n",
      "   </a>\n",
      "   ;\n",
      "and they lived at the bottom of a well.\n",
      "  </p>\n",
      "  <p class=\"story\">\n",
      "   ...\n",
      "  </p>\n",
      " </body>\n",
      "</html>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python27\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 174 of the file c:\\python27\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html_doc)\n",
    "# 按照标准的缩进格式的结构输出\n",
    "print soup.prettify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T06:40:12.011000Z",
     "start_time": "2019-03-26T06:40:11.994000Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.element.Tag'>\n",
      "body\n"
     ]
    }
   ],
   "source": [
    "# Tag\n",
    "tag = soup.body\n",
    "print type(tag)\n",
    "\n",
    "# Name:每个tag都有自己的名字,通过 .name 来获取\n",
    "print tag.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T06:36:48.573000Z",
     "start_time": "2019-03-26T06:36:48.545000Z"
    }
   },
   "source": [
    "### Attributes:一个tag可能有很多个属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T06:41:57.910000Z",
     "start_time": "2019-03-26T06:41:57.883000Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['title']\n",
      "{'class': ['title']}\n",
      "['body', 'strikeout']\n"
     ]
    }
   ],
   "source": [
    "# tag 有一个 “class” 的属性,值为 “boldest” . tag的属性的操作方法与字典相同.\n",
    "tag_p = tag.p\n",
    "print tag_p['class']\n",
    "\n",
    "#也可以直接”点”取属性, 比如: .attrs\n",
    "print tag_p.attrs\n",
    "\n",
    "#多值属性：最常见的多值的属性是 class (一个tag可以有多个CSS的class). 在Beautiful Soup中多值属性的返回类型是list。\n",
    "css_soup = BeautifulSoup('<p class=\"body strikeout\"></p>')\n",
    "print css_soup.p['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T06:41:59.973000Z",
     "start_time": "2019-03-26T06:41:59.938000Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p\n",
      "{'class': ['my', 'id']}\n",
      "['my', 'id']\n",
      "ss\n",
      "{'id': 'my id'}\n"
     ]
    }
   ],
   "source": [
    "id_soup = BeautifulSoup('<p  class=\"my id\">ss</p>')\n",
    "id_soup_1 = BeautifulSoup('<p  id=\"my id\">ss</p>')\n",
    "print id_soup.p.name\n",
    "print id_soup.p.attrs\n",
    "print id_soup.p['class']\n",
    "print id_soup.p.string\n",
    "print id_soup_1.p.attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T06:42:28.816000Z",
     "start_time": "2019-03-26T06:42:28.784000Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extremely bold\n",
      "<class 'bs4.element.NavigableString'>\n"
     ]
    }
   ],
   "source": [
    "# 可以遍历的字符串\n",
    "soup = BeautifulSoup('<b class=\"boldest\">Extremely bold</b>')\n",
    "tag_e = soup.b\n",
    "print tag_e.string\n",
    "print type(tag_e.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个 NavigableString 字符串与Python中的Unicode字符串相同,并且还支持包含在 遍历文档树 和 搜索文档树 中的一些特性. # 通过 unicode() 方法可以直接将 NavigableString 对象转换成Unicode字符串"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T06:42:37.961000Z",
     "start_time": "2019-03-26T06:42:37.930000Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extremely bold\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<b class=\"boldest\">today is not a good day</b>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unicode_str = unicode(tag_e.string)\n",
    "print unicode_str\n",
    "\n",
    "#tag中包含的字符串不能编辑,但是可以被替换成其它的字符串,用 replace_with() 方法\n",
    "tag_e.string.replace_with('today is not a good day')\n",
    "tag_e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注释及特殊字符串**\n",
    "\n",
    "Tag , NavigableString , BeautifulSoup 几乎覆盖了html和xml中的所有内容,但是还有一些特殊对象.容易让人担心的内容是文档的注释部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T06:48:30.309000Z",
     "start_time": "2019-03-26T06:48:30.282000Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.element.Comment'>\n",
      "Hey, buddy. Want to buy a used parser?\n",
      "<b>\n",
      " <!--Hey, buddy. Want to buy a used parser?-->\n",
      "</b>\n"
     ]
    }
   ],
   "source": [
    "markup = \"<b><!--Hey, buddy. Want to buy a used parser?--></b>\"\n",
    "soup = BeautifulSoup(markup)\n",
    "comment = soup.b.string\n",
    "print type(comment)\n",
    "print comment\n",
    "print soup.b.prettify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T06:48:41.282000Z",
     "start_time": "2019-03-26T06:48:41.260000Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      " <head>\n",
      "  <title>\n",
      "   The Dormouse's story\n",
      "  </title>\n",
      " </head>\n",
      " <body>\n",
      "  <p class=\"title\">\n",
      "   <b>\n",
      "    The Dormouse's story\n",
      "   </b>\n",
      "  </p>\n",
      "  <p class=\"story\">\n",
      "   Once upon a time there were three little sisters; and their names were\n",
      "   <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">\n",
      "    Elsie\n",
      "   </a>\n",
      "   ,\n",
      "   <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">\n",
      "    Lacie\n",
      "   </a>\n",
      "   and\n",
      "   <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">\n",
      "    Tillie\n",
      "   </a>\n",
      "   ;\n",
      "and they lived at the bottom of a well.\n",
      "  </p>\n",
      "  <p class=\"story\">\n",
      "   ...\n",
      "  </p>\n",
      " </body>\n",
      "</html>\n",
      "<head><title>The Dormouse's story</title></head>\n",
      "<title>The Dormouse's story</title>\n",
      "<b>The Dormouse's story</b>\n",
      "<b>The Dormouse's story</b>\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(html_doc)\n",
    "print soup.prettify()\n",
    "# tag的名字：操作文档树最简单的方法就是告诉它你想获取的tag的name.如果想获取 <head> 标签,只要用 soup.head \n",
    "print soup.head\n",
    "\n",
    "print soup.title\n",
    "\n",
    "#可以在文档树的tag中多次调用这个方法\n",
    "print soup.body.b\n",
    "print soup.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从结果来看，似乎可以直接从根节点访问嵌套tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T06:48:44.651000Z",
     "start_time": "2019-03-26T06:48:44.638000Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>\n",
      "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>, <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>, <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]\n"
     ]
    }
   ],
   "source": [
    "# 通过点取属性的方式只能获得当前名字的第一个tag\n",
    "print soup.a\n",
    "\n",
    "# 如果想要得到所有的<a>标签,或是通过名字得到比一个tag更多的内容的时候,就需要用到 Searching the tree 中描述的方法,比如: find_all()\n",
    "print soup.find_all('a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### contents 和 .children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T06:48:46.694000Z",
     "start_time": "2019-03-26T06:48:46.657000Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<head><title>The Dormouse's story</title></head>\n",
      "<title>The Dormouse's story</title>\n",
      "[u\"The Dormouse's story\"]\n",
      "--------------------\n",
      "\n",
      "\n",
      "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
      "\n",
      "\n",
      "<p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
      "<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
      "<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a> and\n",
      "<a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>;\n",
      "and they lived at the bottom of a well.</p>\n",
      "\n",
      "\n",
      "<p class=\"story\">...</p>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tag的 .contents 属性可以将tag的子节点以列表的方式输出\n",
    "head_tag = soup.head\n",
    "print head_tag\n",
    "\n",
    "title_tag = head_tag.contents[0]\n",
    "print title_tag\n",
    "print title_tag.contents\n",
    "print '--------------------'\n",
    "for i in soup.body.contents:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "字符串没有 .contents 属性,因为字符串没有子节点\n",
    "\n",
    "\n",
    "通过tag的 .children 生成器,可以对tag的子节点进行循环"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T06:48:49.978000Z",
     "start_time": "2019-03-26T06:48:49.956000Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
      "\n",
      "\n",
      "<p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
      "<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
      "<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a> and\n",
      "<a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>;\n",
      "and they lived at the bottom of a well.</p>\n",
      "\n",
      "\n",
      "<p class=\"story\">...</p>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for child in soup.body.children:\n",
    "    print child"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 爬取[中国证监会机构部基金注册状态](http://ndes.csrc.gov.cn/alappl/home/gongshi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T06:50:15.510000Z",
     "start_time": "2019-03-26T06:50:15.314000Z"
    }
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import urllib2\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')\n",
    "\n",
    "'''发送连接网页的请求'''\n",
    "\n",
    "\n",
    "def get_soup(url, timeout):\n",
    "\n",
    "    request = urllib2.Request(url)\n",
    "    response = urllib2.urlopen(url=request, timeout=timeout)\n",
    "    contents = response.read()\n",
    "    soup = BeautifulSoup(contents, \"html.parser\", from_encoding='utf-8')\n",
    "    return soup\n",
    "\n",
    "'''解析文本信息'''\n",
    "\n",
    "\n",
    "def from_text_to_news(text, recode_dict):\n",
    "    company = re.compile('^关于(.*?)的').findall(text)[0]\n",
    "    key_new = re.compile('.*《(.*)》').findall(text)[0].split('—')\n",
    "    event = key_new[0]\n",
    "    if len(key_new) == 2:\n",
    "        fund_name = key_new[1]\n",
    "        recode_dict['基金名称'] = fund_name\n",
    "    elif len(key_new) == 3 and key_new[1] == '':\n",
    "        fund_name = key_new[2]\n",
    "        recode_dict['基金名称'] = fund_name\n",
    "\n",
    "    recode_dict['公司名称'] = company\n",
    "    recode_dict['申请注册事项'] = event\n",
    "\n",
    "    return recode_dict\n",
    "\n",
    "\n",
    "''' 获取一页的所有信息 '''\n",
    "\n",
    "\n",
    "def page_news(useful_url, page, timeout, page_news_list, recode_list):\n",
    "    useful_url_page = useful_url + '&' + 'pageNo=' + str(page)\n",
    "    soup_1 = get_soup(url=useful_url_page, timeout=timeout)\n",
    "\n",
    "    for child in soup_1.find_all('div', attrs={'style': 'margin-top: -11px; margin-left: 12px;'}):\n",
    "        recode_dict = {i: '' for i in recode_list}\n",
    "\n",
    "        recode_dict['page_seq'] = page\n",
    "        text = str(child.find('div', attrs={'class': 'titleshow'}).string[:])\n",
    "\n",
    "        recode_dict = from_text_to_news(text, recode_dict=recode_dict)\n",
    "\n",
    "        for child_0 in child.table.find_all('tr'):\n",
    "            if child_0.attrs == {}:\n",
    "                child_set = child_0.find_all('td')\n",
    "                process_name = str([i for i in child_set[0].stripped_strings][0])\n",
    "                process_time = [i for i in child_set[1].stripped_strings][0]\n",
    "                recode_dict[process_name] = process_time\n",
    "        page_news_list.append(recode_dict)\n",
    "    return page_news_list\n",
    "\n",
    "'''获取网站上所有页码的信息'''\n",
    "\n",
    "\n",
    "def get_useful_news(url, timeout, part_name, recode_list):\n",
    "    soup = get_soup(url=url, timeout=timeout)\n",
    "    useful_url = soup.find('div', text=part_name).parent['href']\n",
    "    # 获取总页数\n",
    "    page_soup = get_soup(url=useful_url, timeout=timeout)\n",
    "    page_num = int(re.findall(r\"\\d+\\.?\\d*\", page_soup.find('span', attrs={'class': 'jump_text'}).string)[0])\n",
    "\n",
    "    news_list = []\n",
    "    for page in tqdm(xrange(1, page_num+1)):\n",
    "        page_news_list = []\n",
    "        page_news_list = page_news(useful_url=useful_url, page=page, timeout=timeout, page_news_list=page_news_list, recode_list=recode_list)\n",
    "        news_list.extend(page_news_list)\n",
    "    return news_list\n",
    "\n",
    "'''初始化，全量爬虫'''\n",
    "\n",
    "\n",
    "def initialization_information(recode_list, url, initialization_path):\n",
    "    news_list = get_useful_news(url=url, timeout=100, part_name='机构部', recode_list=recode_list)\n",
    "    news_pd = pd.DataFrame(map(lambda x: [x[i] for i in recode_list], news_list), columns=recode_list)\n",
    "    news_pd.to_csv(initialization_path)\n",
    "    return initialization_path\n",
    "\n",
    "###########################################\n",
    "# 主程序\n",
    "# import time\n",
    "# import pandas as pd\n",
    "# start =time.time()\n",
    "# if __name__ == '__main__':\n",
    "#     recode_list = ['公司名称', '申请注册事项', '基金名称', '接收材料', '补正通知', '受理通知', '一次书面反馈', '行政许可决定书', 'page_seq']\n",
    "\n",
    "#     url = \"http://ndes.csrc.gov.cn/alappl/home/gongshi\"\n",
    "\n",
    "#     '''路径存放关键字为当前日期'''\n",
    "#     initialization_path = 'news_' + time.strftime(\"%Y-%d-%m\") + '.csv'\n",
    "#     '''开始全量爬虫'''\n",
    "#     initialization_information(recode_list=recode_list, url=url, initialization_path=initialization_path)\n",
    "\n",
    "#     end = time.time()\n",
    "#     time_diff = (end - start)/3600\n",
    "#     print('Running time: %f Hours'%time_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 爬取市政局[高架封路数据](http://www.highway.sh.cn/#/website/public/closeRoad.html)和[夜间施工数据](http://www.highway.sh.cn/#/website/public/nightRecord.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T06:59:54.629000Z",
     "start_time": "2019-03-26T06:59:54.268000Z"
    }
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import time\n",
    "import sys\n",
    "import requests, json\n",
    "from datetime import datetime\n",
    "from dateutil.parser import parse\n",
    "import pandas as pd\n",
    "\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')\n",
    "\n",
    "'''发送连接网页的请求'''\n",
    "\n",
    "\n",
    "def get_night_record_data(url, pageSize, currentPage):\n",
    "    assert pageSize == 10 or pageSize == 30\n",
    "    res = requests.post(url, data={\"currentPage\": currentPage, \"pageSize\": pageSize}).text\n",
    "    data = json.loads(res)\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_gaojia_fenglu_data(url, date):\n",
    "    res = requests.post(url=url, data={\"CLOSEDATE\": date}).text\n",
    "    return json.loads(res)\n",
    "\n",
    "\n",
    "\n",
    "'''解析文本信息'''\n",
    "\n",
    "\n",
    "def resolve_night_json(json_data, recode_dict):\n",
    "    values_dict = {recode_dict[i]: [] for i in recode_dict}\n",
    "    for slice_news in json_data['rows']:\n",
    "        for j in recode_dict:\n",
    "            if j != 'begin_date_time' and j != 'end_date_time':\n",
    "                values_dict[recode_dict[j]].append(slice_news[j])\n",
    "            elif j is 'begin_date_time':\n",
    "                values_dict[recode_dict[j]].append(parse('-'.join([str(slice_news['begindate']).split(' ')[0], str(slice_news['begintime'])])))\n",
    "            elif j is 'end_date_time':\n",
    "                values_dict[recode_dict[j]].append(parse('-'.join([str(slice_news['enddate']).split(' ')[0], str(slice_news['endtime'])])))\n",
    "    slice_df = pd.DataFrame(values_dict)\n",
    "    return slice_df\n",
    "\n",
    "\n",
    "def night_initialization_information(night_url, pageSize, night_recode_dict, night_path):\n",
    "\n",
    "    json_news_len = 1\n",
    "    json_news_list = []\n",
    "    page_seq = 1\n",
    "\n",
    "    while json_news_len != 0:\n",
    "        print page_seq\n",
    "        json_data = get_night_record_data(url=night_url, pageSize=pageSize, currentPage=page_seq)\n",
    "        json_news_len = len(json_data['rows'])\n",
    "        slice_df = resolve_night_json(json_data=json_data, recode_dict=night_recode_dict)\n",
    "        slice_df['page_seq'] = page_seq\n",
    "        json_news_list.append(slice_df)\n",
    "        page_seq += 1\n",
    "    night_road_df = pd.concat(json_news_list, axis=0)\n",
    "    night_road_df.to_csv(night_path, index=False)\n",
    "    return night_road_df\n",
    "\n",
    "\n",
    "def resolve_gaojia_json(json_data, recode_dict, key_columns):\n",
    "    values_dict = {recode_dict[i]: [] for i in recode_dict}\n",
    "    for slice_news in json_data:\n",
    "        bool_list = map(lambda x: False if x not in slice_news else (slice_news[x] in values_dict[recode_dict[x]]), key_columns)\n",
    "        if not len(bool_list) == sum(bool_list):\n",
    "            for j in recode_dict:\n",
    "                values_dict[recode_dict[j]].append(None if j not in slice_news else slice_news[j])\n",
    "    slice_df = pd.DataFrame(values_dict)\n",
    "    return slice_df\n",
    "\n",
    "\n",
    "def gaojia_initialization_information(gaojia_url, gaojia_recode_dict, begin_date, end_date, gaojia_path, key_columns):\n",
    "    date_list = [datetime.strftime(i, '%Y-%m-%d') for i in pd.date_range(start=begin_date, end=end_date)]\n",
    "    gaojia_news_list = []\n",
    "    for i in date_list:\n",
    "        print i\n",
    "        data = get_gaojia_fenglu_data(url=gaojia_url, date=i)\n",
    "        slice_df = resolve_gaojia_json(json_data=data, recode_dict=gaojia_recode_dict, key_columns=key_columns)\n",
    "        gaojia_news_list.append(slice_df)\n",
    "    gaojia_road_df = pd.concat(gaojia_news_list, axis=0)\n",
    "    gaojia_road_df.to_csv(gaojia_path, index=False)\n",
    "    return gaojia_road_df\n",
    "\n",
    "'''\n",
    "if __name__=='__main__':\n",
    "    gaojia_url = \"http://www.highway.sh.cn/web/gjfl/search.action\"\n",
    "    gaojia_recode_dict = {'CLOSE_AREA': '封路区域', 'CLOSE_CM': '方向', 'CLOSE_CONTENT': '路段区域',\n",
    "                         'CLOSE_DATE': '日期', 'CLOSE_END_TIME': '起始封路时间', 'CLOSE_START_TIME': '结束封路时间',\n",
    "                         'ROAD_NAME': '高架名称', 'IMAGE_URL': '示意图网址'}\n",
    "    key_columns = ['CLOSE_DATE', 'CLOSE_CONTENT']\n",
    "    gaojia_path = night_road_path = 'output/nightRecord_' + time.strftime(\"%Y%m%d\") + '.csv'\n",
    "    gaojia_initialization_information(gaojia_url=gaojia_url, gaojia_recode_dict=gaojia_recode_dict, begin_date='20180101',\n",
    "                                      end_date=time.strftime(\"%Y%m%d\"), gaojia_path=gaojia_path, key_columns=key_columns)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T07:08:14.588000Z",
     "start_time": "2019-03-26T07:08:13.988000Z"
    }
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import time\n",
    "import sys\n",
    "import requests, json\n",
    "from datetime import datetime\n",
    "from dateutil.parser import parse\n",
    "import pandas as pd\n",
    "\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')\n",
    "\n",
    "'''发送连接网页的请求'''\n",
    "\n",
    "res = requests.post(url='http://www.ks121.com/').text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## python 画图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**循环画子图**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`import matplotlib.pyplot as plt\n",
    "    for i in range(len(stationID_list)):\n",
    "        if i%4 == 0:\n",
    "            fig = plt.figure(figsize=(10, 10))\n",
    "        ax = fig.add_subplot(2, 2, i%4+1)\n",
    "        id_df = merge_data[merge_data['stationID'] == stationID_list[i]].reset_index()\n",
    "        l = ax.plot(id_df.index, id_df['inNums_{}'.format(day1)], 'green', id_df.index, id_df['inNums_{}'.format(day2)],'red')\n",
    "        plt.xlabel(\"time_of_id_{}\".format(stationID_list[i]))\n",
    "        plt.ylabel('{}Day_to_{}Day'.format(tianqi_dict[day1], tianqi_dict[day2]))\n",
    "        plt.legend(handles=l, labels=[str(day1), str(day2)], loc='best')\n",
    "        if i%4 == 3 or i == len(stationID_list)-1:\n",
    "            fig.savefig('figure/inNumTrend_{}Day_to_{}Day_endStation{}'.format(tianqi_dict[day1], tianqi_dict[day2],i))\n",
    "    plt.close()\n",
    "    return 0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
